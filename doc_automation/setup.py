#!/usr/bin/env python3
"""
æ–‡æ¡£è‡ªåŠ¨åŒ–ç³»ç»Ÿå¿«é€Ÿè®¾ç½®è„šæœ¬
ä¸€é”®é…ç½®å®Œæ•´çš„ RAG é©±åŠ¨çš„æ–‡æ¡£ç”Ÿæˆæµæ°´çº¿
"""

import os
import shutil
import subprocess
import sys
from pathlib import Path


def create_directory_structure():
    """åˆ›å»ºç›®å½•ç»“æ„"""
    directories = [
        'doc_automation/core',
        'doc_automation/rag',
        'doc_automation/data',
        'doc_automation/templates',
        'doc_automation/output',
        'doc_automation/config',
        'doc_automation/tests',
        'docusaurus_site/docs',
        'docusaurus_site/static',
        'dagster_storage'
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"âœ… åˆ›å»ºç›®å½•: {directory}")


def create_core_modules():
    """åˆ›å»ºæ ¸å¿ƒæ¨¡å—"""
    
    # æ–‡æ¡£ç”Ÿæˆå™¨
    doc_generator_code = '''"""
æ–‡æ¡£ç”Ÿæˆå™¨æ ¸å¿ƒæ¨¡å—
åŸºäº RAG å’Œ DocSpec è§„èŒƒç”Ÿæˆå„ç±»æ–‡æ¡£
"""

import yaml
import json
from typing import Dict, Any, List
from jinja2 import Environment, FileSystemLoader
from .rag_retriever import RAGRetriever


class DocumentGenerator:
    def __init__(self, rag_retriever: RAGRetriever):
        self.rag_retriever = rag_retriever
        self.jinja_env = Environment(
            loader=FileSystemLoader('templates'),
            autoescape=True
        )
    
    def generate(self, docspec: Dict[str, Any], output_format: str) -> str:
        """æ ¹æ®æ–‡æ¡£è§„æ ¼ç”Ÿæˆæ–‡æ¡£"""
        doc_type = docspec['type']
        sections = docspec['sections']
        
        # æ„å»ºæ–‡æ¡£å†…å®¹
        content_parts = []
        
        # ç”Ÿæˆå…ƒæ•°æ®å¤´éƒ¨
        metadata = self._generate_metadata(docspec['metadata'])
        content_parts.append(metadata)
        
        # ç”Ÿæˆå„ä¸ªç« èŠ‚
        for section in sections:
            section_content = self._generate_section(section)
            content_parts.append(section_content)
        
        # åˆå¹¶å†…å®¹
        full_content = '\\n\\n'.join(content_parts)
        
        # æ ¹æ®è¾“å‡ºæ ¼å¼è½¬æ¢
        if output_format == 'markdown':
            return full_content
        elif output_format == 'docx':
            return self._convert_to_docx(full_content)
        elif output_format == 'html':
            return self._convert_to_html(full_content)
        else:
            return full_content
    
    def _generate_metadata(self, metadata: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡æ¡£å…ƒæ•°æ®"""
        title = metadata.get('title', 'Untitled Document')
        author = metadata.get('author', 'System Generated')
        
        return f"""# {title}

**ä½œè€…:** {author}  
**ç”Ÿæˆæ—¶é—´:** {metadata.get('created_at', 'Unknown')}  
**ç‰ˆæœ¬:** {metadata.get('version', '1.0.0')}

---
"""
    
    def _generate_section(self, section: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡æ¡£ç« èŠ‚"""
        section_id = section['id']
        section_title = section['title']
        section_type = section['type']
        
        content = f"## {section_title}\\n\\n"
        
        if section_type == 'text':
            # ç”Ÿæˆæ–‡æœ¬å†…å®¹
            content += self._generate_text_content(section)
        elif section_type == 'table':
            # ç”Ÿæˆè¡¨æ ¼
            content += self._generate_table_content(section)
        elif section_type == 'chart':
            # ç”Ÿæˆå›¾è¡¨
            content += self._generate_chart_content(section)
        elif section_type == 'flowchart':
            # ç”Ÿæˆæµç¨‹å›¾
            content += self._generate_flowchart_content(section)
        
        return content
    
    def _generate_text_content(self, section: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡æœ¬å†…å®¹"""
        prompts = section.get('prompts', [])
        data_sources = section.get('data_sources', [])
        
        # ä½¿ç”¨ RAG æ£€ç´¢ç›¸å…³ä¿¡æ¯
        context = ""
        for data_source in data_sources:
            relevant_data = self.rag_retriever.retrieve(data_source, limit=5)
            context += f"\\n{relevant_data}"
        
        # åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå†…å®¹
        if prompts:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨ LLM ç”Ÿæˆå†…å®¹
            generated_content = self._call_llm_for_generation(prompts[0], context)
            return generated_content
        
        return "å†…å®¹å¾…ç”Ÿæˆ..."
    
    def _generate_table_content(self, section: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¡¨æ ¼å†…å®¹"""
        schema = section.get('table_schema', {})
        columns = schema.get('columns', [])
        
        # æ„å»º Markdown è¡¨æ ¼
        header = "| " + " | ".join([col['name'] for col in columns]) + " |"
        separator = "| " + " | ".join(['---'] * len(columns)) + " |"
        
        table_content = f"{header}\\n{separator}\\n"
        
        # è·å–æ•°æ®å¹¶å¡«å……è¡¨æ ¼
        data_sources = section.get('data_sources', [])
        for data_source in data_sources:
            table_data = self.rag_retriever.get_structured_data(data_source)
            for row in table_data[:10]:  # é™åˆ¶è¡Œæ•°
                row_content = "| " + " | ".join([str(row.get(col['name'], '')) for col in columns]) + " |"
                table_content += f"{row_content}\\n"
        
        return table_content
    
    def _generate_chart_content(self, section: Dict[str, Any]) -> str:
        """ç”Ÿæˆå›¾è¡¨å†…å®¹"""
        chart_config = section.get('chart_config', {})
        chart_type = chart_config.get('type', 'bar')
        
        # ç”Ÿæˆå›¾è¡¨æè¿°å’Œé…ç½®
        chart_description = f"### {chart_type.title()} å›¾è¡¨\\n\\n"
        chart_description += "```json\\n"
        chart_description += json.dumps(chart_config, indent=2, ensure_ascii=False)
        chart_description += "\\n```\\n\\n"
        
        return chart_description
    
    def _generate_flowchart_content(self, section: Dict[str, Any]) -> str:
        """ç”Ÿæˆæµç¨‹å›¾å†…å®¹"""
        mermaid_template = section.get('mermaid_template', '')
        
        if mermaid_template:
            return f"```mermaid\\n{mermaid_template}\\n```\\n\\n"
        
        return "æµç¨‹å›¾å¾…ç”Ÿæˆ..."
    
    def _call_llm_for_generation(self, prompt: str, context: str) -> str:
        """è°ƒç”¨ LLM ç”Ÿæˆå†…å®¹"""
        # è¿™é‡Œåº”è¯¥é›†æˆå®é™…çš„ LLM API è°ƒç”¨
        # æ¯”å¦‚ DashScopeã€OpenAI ç­‰
        return f"åŸºäºæç¤º '{prompt}' å’Œä¸Šä¸‹æ–‡ç”Ÿæˆçš„å†…å®¹..."
    
    def _convert_to_docx(self, markdown_content: str) -> str:
        """è½¬æ¢ä¸º DOCX æ ¼å¼"""
        # ä½¿ç”¨ python-docx æˆ– pandoc è½¬æ¢
        return f"[DOCX] {markdown_content}"
    
    def _convert_to_html(self, markdown_content: str) -> str:
        """è½¬æ¢ä¸º HTML æ ¼å¼"""
        # ä½¿ç”¨ markdown åº“è½¬æ¢
        return f"<html><body>{markdown_content}</body></html>"
'''
    
    with open('doc_automation/core/doc_generator.py', 'w', encoding='utf-8') as f:
        f.write(doc_generator_code)
    
    # RAG æ£€ç´¢å™¨
    rag_retriever_code = '''"""
RAG æ£€ç´¢å™¨æ¨¡å—
æ”¯æŒå¤šç§é«˜çº§æ£€ç´¢ç­–ç•¥
"""

from typing import Dict, Any, List
import numpy as np
from sentence_transformers import SentenceTransformer


class RAGRetriever:
    def __init__(self):
        self.embedding_model = SentenceTransformer('BAAI/bge-m3')
        self.knowledge_base = {}
        self.embeddings = {}
    
    def index_data(self, source_name: str, data: Dict[str, Any]):
        """ç´¢å¼•æ•°æ®åˆ°çŸ¥è¯†åº“"""
        self.knowledge_base[source_name] = data
        
        # ç”ŸæˆåµŒå…¥å‘é‡
        if isinstance(data, dict):
            text_content = self._extract_text_from_dict(data)
        else:
            text_content = str(data)
        
        embedding = self.embedding_model.encode(text_content)
        self.embeddings[source_name] = embedding
        
        print(f"âœ… å·²ç´¢å¼•æ•°æ®æº: {source_name}")
    
    def retrieve(self, query: str, limit: int = 5) -> str:
        """æ£€ç´¢ç›¸å…³ä¿¡æ¯"""
        query_embedding = self.embedding_model.encode(query)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = {}
        for source_name, embedding in self.embeddings.items():
            similarity = np.dot(query_embedding, embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(embedding)
            )
            similarities[source_name] = similarity
        
        # æ’åºå¹¶è¿”å›æœ€ç›¸å…³çš„å†…å®¹
        sorted_sources = sorted(similarities.items(), key=lambda x: x[1], reverse=True)
        
        result_content = []
        for source_name, score in sorted_sources[:limit]:
            if score > 0.5:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                content = self.knowledge_base[source_name]
                result_content.append(f"**{source_name}** (ç›¸ä¼¼åº¦: {score:.3f})\\n{content}")
        
        return "\\n\\n".join(result_content)
    
    def get_structured_data(self, source_name: str) -> List[Dict[str, Any]]:
        """è·å–ç»“æ„åŒ–æ•°æ®"""
        data = self.knowledge_base.get(source_name, {})
        
        if isinstance(data, list):
            return data
        elif isinstance(data, dict):
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            return [data]
        else:
            return []
    
    def _extract_text_from_dict(self, data: Dict[str, Any]) -> str:
        """ä»å­—å…¸ä¸­æå–æ–‡æœ¬å†…å®¹"""
        text_parts = []
        
        def extract_recursive(obj, prefix=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    extract_recursive(value, f"{prefix}{key}: ")
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    extract_recursive(item, f"{prefix}[{i}] ")
            else:
                text_parts.append(f"{prefix}{str(obj)}")
        
        extract_recursive(data)
        return " ".join(text_parts)
'''
    
    with open('doc_automation/core/rag_retriever.py', 'w', encoding='utf-8') as f:
        f.write(rag_retriever_code)
    
    # æ•°æ®è·å–å™¨
    data_fetcher_code = '''"""
æ•°æ®è·å–å™¨æ¨¡å—
ä»å„ç§æ•°æ®æºè·å–å®¡æ ¸ç›¸å…³æ•°æ®
"""

import random
from datetime import datetime, timedelta
from typing import Dict, Any


class DataFetcher:
    def __init__(self):
        self.mock_data = True  # ç”Ÿäº§ç¯å¢ƒä¸­åº”è¿æ¥çœŸå®æ•°æ®æº
    
    def get_audit_metrics(self) -> Dict[str, Any]:
        """è·å–å®¡æ ¸æ€§èƒ½æŒ‡æ ‡"""
        if self.mock_data:
            return {
                'total_processed': random.randint(10000, 50000),
                'accuracy_rate': round(random.uniform(0.92, 0.98), 3),
                'false_positive_rate': round(random.uniform(0.01, 0.03), 3),
                'false_negative_rate': round(random.uniform(0.005, 0.015), 3),
                'avg_processing_time': f"{random.randint(15, 45)}s",
                'escalation_ratio': round(random.uniform(0.02, 0.08), 3),
                'daily_trends': [
                    {'date': (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d'),
                     'volume': random.randint(1000, 3000),
                     'accuracy': round(random.uniform(0.90, 0.98), 3)}
                    for i in range(7)
                ]
            }
        
        # è¿™é‡Œåº”è¯¥è¿æ¥å®é™…çš„æ•°æ®åº“æˆ– API
        return {}
    
    def get_experiment_data(self) -> Dict[str, Any]:
        """è·å–å®éªŒæ•°æ®"""
        if self.mock_data:
            return {
                'ab_tests': [
                    {
                        'experiment_id': 'exp_001',
                        'name': 'Promptä¼˜åŒ–å®éªŒ',
                        'start_date': '2024-01-01',
                        'end_date': '2024-01-15',
                        'groups': {
                            'control': {'accuracy': 0.92, 'latency': 45},
                            'treatment': {'accuracy': 0.95, 'latency': 42}
                        },
                        'statistical_significance': 0.95
                    }
                ],
                'model_versions': [
                    {
                        'version': 'v2.1.0',
                        'release_date': '2024-01-10',
                        'improvements': ['æå‡å‡†ç¡®ç‡3%', 'é™ä½å»¶è¿Ÿ10%'],
                        'performance_delta': {'accuracy': 0.03, 'latency': -0.1}
                    }
                ]
            }
        
        return {}
    
    def get_policy_configs(self) -> Dict[str, Any]:
        """è·å–ç­–ç•¥é…ç½®æ•°æ®"""
        if self.mock_data:
            return {
                'risk_categories': [
                    {'name': 'æš´åŠ›å†…å®¹', 'threshold': 0.8, 'action': 'block'},
                    {'name': 'è‰²æƒ…å†…å®¹', 'threshold': 0.7, 'action': 'review'},
                    {'name': 'è™šå‡ä¿¡æ¯', 'threshold': 0.6, 'action': 'flag'}
                ],
                'content_types': [
                    {'type': 'çŸ­è§†é¢‘', 'daily_volume': 50000, 'auto_rate': 0.85},
                    {'type': 'å›¾æ–‡', 'daily_volume': 30000, 'auto_rate': 0.90},
                    {'type': 'ç›´æ’­', 'daily_volume': 5000, 'auto_rate': 0.70}
                ],
                'escalation_rules': [
                    {'condition': 'confidence < 0.5', 'action': 'human_review'},
                    {'condition': 'risk_score > 0.9', 'action': 'expert_review'}
                ]
            }
        
        return {}
    
    def get_user_feedback(self) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·åé¦ˆæ•°æ®"""
        if self.mock_data:
            return {
                'satisfaction_scores': [
                    {'department': 'å†…å®¹å®‰å…¨', 'score': 4.2, 'responses': 156},
                    {'department': 'äº§å“è¿è¥', 'score': 3.8, 'responses': 89},
                    {'department': 'ç®—æ³•å›¢é˜Ÿ', 'score': 4.5, 'responses': 67}
                ],
                'feature_requests': [
                    {'feature': 'æ‰¹é‡å®¡æ ¸', 'votes': 23, 'priority': 'high'},
                    {'feature': 'è‡ªå®šä¹‰è§„åˆ™', 'votes': 18, 'priority': 'medium'},
                    {'feature': 'ç§»åŠ¨ç«¯æ”¯æŒ', 'votes': 12, 'priority': 'low'}
                ],
                'pain_points': [
                    'å®¡æ ¸ç•Œé¢å“åº”æ…¢',
                    'è¯¯æŠ¥ç‡åé«˜',
                    'åŸ¹è®­ææ–™ä¸è¶³'
                ]
            }
        
        return {}
'''
    
    with open('doc_automation/core/data_fetcher.py', 'w', encoding='utf-8') as f:
        f.write(data_fetcher_code)
    
    print("âœ… æ ¸å¿ƒæ¨¡å—åˆ›å»ºå®Œæˆ")


def create_dagster_config():
    """åˆ›å»º Dagster é…ç½®æ–‡ä»¶"""
    dagster_yaml = '''# Dagster å·¥ä½œç©ºé—´é…ç½®
load_from:
  - python_package: doc_automation.pipelines

telemetry:
  enabled: false

storage:
  filesystem:
    base_dir: dagster_storage

run_launcher:
  module: dagster._core.launcher.sync_in_memory_run_launcher
  class: SyncInMemoryRunLauncher

compute_logs:
  module: dagster._core.storage.local_compute_log_manager
  class: LocalComputeLogManager
  config:
    base_dir: dagster_storage/compute_logs
'''
    
    with open('doc_automation/workspace.yaml', 'w', encoding='utf-8') as f:
        f.write(dagster_yaml)
    
    print("âœ… Dagster é…ç½®åˆ›å»ºå®Œæˆ")


def create_api_server():
    """åˆ›å»º FastAPI æœåŠ¡å™¨"""
    api_code = '''"""
æ–‡æ¡£ç”Ÿæˆ API æœåŠ¡å™¨
æä¾› REST API æ¥å£è§¦å‘æ–‡æ¡£ç”Ÿæˆ
"""

from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
from typing import List, Dict, Any
import uvicorn
import asyncio
from dagster import execute_job_sync
from .pipelines.doc_generation_pipeline import doc_generation_job


app = FastAPI(title="æ–‡æ¡£è‡ªåŠ¨åŒ– API", version="1.0.0")


class DocGenerationRequest(BaseModel):
    doc_type: str
    output_formats: List[str] = ["markdown", "docx"]
    auto_publish: bool = True
    context: Dict[str, Any] = {}


class DocGenerationResponse(BaseModel):
    job_id: str
    status: str
    message: str


@app.post("/api/doc-automation/generate", response_model=DocGenerationResponse)
async def generate_document(
    request: DocGenerationRequest,
    background_tasks: BackgroundTasks
):
    """è§¦å‘æ–‡æ¡£ç”Ÿæˆä»»åŠ¡"""
    
    # ç”Ÿæˆä»»åŠ¡ID
    job_id = f"doc_gen_{request.doc_type}_{int(asyncio.get_event_loop().time())}"
    
    # æ„å»º Dagster ä½œä¸šé…ç½®
    run_config = {
        "ops": {
            "generate_documents": {
                "config": {
                    "doc_type": request.doc_type,
                    "output_formats": request.output_formats,
                    "auto_publish": request.auto_publish
                }
            }
        }
    }
    
    # åœ¨åå°æ‰§è¡Œ Dagster ä½œä¸š
    background_tasks.add_task(run_dagster_job, run_config)
    
    return DocGenerationResponse(
        job_id=job_id,
        status="started",
        message=f"æ–‡æ¡£ç”Ÿæˆä»»åŠ¡å·²å¯åŠ¨: {request.doc_type}"
    )


async def run_dagster_job(run_config: Dict[str, Any]):
    """åœ¨åå°è¿è¡Œ Dagster ä½œä¸š"""
    try:
        result = execute_job_sync(doc_generation_job, run_config=run_config)
        print(f"âœ… Dagster ä½œä¸šæ‰§è¡Œå®Œæˆ: {result.run_id}")
    except Exception as e:
        print(f"âŒ Dagster ä½œä¸šæ‰§è¡Œå¤±è´¥: {e}")


@app.get("/api/doc-automation/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "message": "æ–‡æ¡£è‡ªåŠ¨åŒ–æœåŠ¡è¿è¡Œæ­£å¸¸"}


@app.get("/api/doc-automation/docs")
async def list_documents():
    """åˆ—å‡ºå·²ç”Ÿæˆçš„æ–‡æ¡£"""
    # è¿™é‡Œåº”è¯¥æ‰«æè¾“å‡ºç›®å½•æˆ–æŸ¥è¯¢æ•°æ®åº“
    return {"documents": []}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
'''
    
    with open('doc_automation/api_server.py', 'w', encoding='utf-8') as f:
        f.write(api_code)
    
    print("âœ… API æœåŠ¡å™¨åˆ›å»ºå®Œæˆ")


def create_start_script():
    """åˆ›å»ºå¯åŠ¨è„šæœ¬"""
    start_script = '''#!/bin/bash

echo "ğŸš€ å¯åŠ¨æ–‡æ¡£è‡ªåŠ¨åŒ–ç³»ç»Ÿ..."

# æ£€æŸ¥ Python ä¾èµ–
echo "ğŸ“¦ æ£€æŸ¥ä¾èµ–..."
pip install -r requirements.txt

# åˆå§‹åŒ– Dagster
echo "âš™ï¸ åˆå§‹åŒ– Dagster..."
export DAGSTER_HOME=$(pwd)/dagster_storage
dagster instance migrate

# å¯åŠ¨ Dagster UI (åå°)
echo "ğŸ¯ å¯åŠ¨ Dagster UI..."
nohup dagster dev --host 0.0.0.0 --port 3000 > dagster.log 2>&1 &

# å¯åŠ¨ API æœåŠ¡å™¨ (åå°)
echo "ğŸŒ å¯åŠ¨ API æœåŠ¡å™¨..."
nohup python -m doc_automation.api_server > api.log 2>&1 &

echo "âœ… ç³»ç»Ÿå¯åŠ¨å®Œæˆï¼"
echo ""
echo "ğŸ“Š Dagster UI: http://localhost:3000"
echo "ğŸ”Œ API æ¥å£: http://localhost:8000"
echo "ğŸ“– API æ–‡æ¡£: http://localhost:8000/docs"
echo ""
echo "æ—¥å¿—æ–‡ä»¶:"
echo "  - Dagster: dagster.log"
echo "  - API: api.log"
echo ""
echo "åœæ­¢æœåŠ¡: ./stop.sh"
'''
    
    with open('doc_automation/start.sh', 'w', encoding='utf-8') as f:
        f.write(start_script)
    os.chmod('doc_automation/start.sh', 0o755)
    
    # åˆ›å»ºåœæ­¢è„šæœ¬
    stop_script = '''#!/bin/bash

echo "ğŸ›‘ åœæ­¢æ–‡æ¡£è‡ªåŠ¨åŒ–ç³»ç»Ÿ..."

# åœæ­¢æ‰€æœ‰ç›¸å…³è¿›ç¨‹
pkill -f "dagster dev"
pkill -f "api_server"

echo "âœ… ç³»ç»Ÿå·²åœæ­¢"
'''
    
    with open('doc_automation/stop.sh', 'w', encoding='utf-8') as f:
        f.write(stop_script)
    os.chmod('doc_automation/stop.sh', 0o755)
    
    print("âœ… å¯åŠ¨è„šæœ¬åˆ›å»ºå®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ—ï¸ å¼€å§‹è®¾ç½®æ–‡æ¡£è‡ªåŠ¨åŒ–ç³»ç»Ÿ...")
    
    try:
        create_directory_structure()
        create_core_modules()
        create_dagster_config()
        create_api_server()
        create_start_script()
        
        print("\nğŸ‰ æ–‡æ¡£è‡ªåŠ¨åŒ–ç³»ç»Ÿè®¾ç½®å®Œæˆï¼")
        print("\nä¸‹ä¸€æ­¥æ“ä½œ:")
        print("1. å®‰è£…ä¾èµ–: pip install -r requirements.txt")
        print("2. å¯åŠ¨ç³»ç»Ÿ: ./doc_automation/start.sh")
        print("3. è®¿é—® Dagster UI: http://localhost:3000")
        print("4. æµ‹è¯• API: http://localhost:8000/docs")
        print("\nğŸ”— å‰ç«¯é›†æˆ:")
        print("- åœ¨å®¡æ ¸ç•Œé¢ç‚¹å‡»æ–‡æ¡£ç”ŸæˆæŒ‰é’®")
        print("- è°ƒç”¨ POST /api/doc-automation/generate")
        
    except Exception as e:
        print(f"âŒ è®¾ç½®å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
